<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="false" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<!-- 数学公式渲染（KaTeX 自动识别 $...$ 和 $$...$$） -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '\\[', right: '\\]', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false}
    ]
  });"></script>

<head>
  <meta charset="utf-8">
  
  
  

  
  <title>自监督式学习 | 超级迪多的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="Self-supervised Learning supervised Learning: model的输入是x，输出是y；为了使model有想要的y，我们需要label。 Self-supervised Learning：自己想办法做supervise，在没有label的情况下。在没有标注的资料里面，将资料分成两部分，让一部分作为模型的输入，另一部分作为模型的标注。  有label就是super">
<meta property="og:type" content="article">
<meta property="og:title" content="自监督式学习">
<meta property="og:url" content="https://diduox.github.io/2025/11/04/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="超级迪多的博客">
<meta property="og:description" content="Self-supervised Learning supervised Learning: model的输入是x，输出是y；为了使model有想要的y，我们需要label。 Self-supervised Learning：自己想办法做supervise，在没有label的情况下。在没有标注的资料里面，将资料分成两部分，让一部分作为模型的输入，另一部分作为模型的标注。  有label就是super">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102144607127.png">
<meta property="og:image" content="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102150433520.png">
<meta property="og:image" content="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102151859368.png">
<meta property="og:image" content="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102154437006.png">
<meta property="og:image" content="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102155914445.png">
<meta property="og:image" content="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102161233548.png">
<meta property="og:image" content="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102165119435.png">
<meta property="og:image" content="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251104171233458.png">
<meta property="og:image" content="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251104171805144.png">
<meta property="og:image" content="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251104172713698.png">
<meta property="og:image" content="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251104173554388.png">
<meta property="article:published_time" content="2025-11-04T09:44:44.000Z">
<meta property="article:modified_time" content="2025-11-04T09:44:23.645Z">
<meta property="article:author" content="迪多">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102144607127.png">
  
    <link rel="alternate" href="/atom.xml" title="超级迪多的博客" type="application/atom+xml">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  
   
  <div id="main-grid" class="  ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>超级迪多的博客 </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
      <a id="nav-rss-link" class="nav-icon mobile-hide" href="/atom.xml" title="RSS 订阅">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M198-120q-25.846 0-44.23-18.384-18.384-18.385-18.384-44.23 0-25.846 18.384-44.23 18.384-18.385 44.23-18.385 25.846 0 44.23 18.385 18.384 18.384 18.384 44.23 0 25.845-18.384 44.23Q223.846-120 198-120Zm538.385 0q-18.846 0-32.923-13.769-14.076-13.769-15.922-33.23-8.692-100.616-51.077-188.654-42.385-88.039-109.885-155.539-67.5-67.501-155.539-109.885Q283-663.462 182.385-672.154q-19.461-1.846-33.23-16.23-13.769-14.385-13.769-33.846t14.076-32.922q14.077-13.461 32.923-12.23 120.076 8.692 226.038 58.768 105.961 50.077 185.73 129.846 79.769 79.769 129.846 185.731 50.077 105.961 58.769 226.038 1.231 18.846-12.538 32.922Q756.461-120 736.385-120Zm-252 0q-18.231 0-32.423-13.461t-18.653-33.538Q418.155-264.23 348.886-333.5q-69.27-69.27-166.501-84.423-20.077-4.462-33.538-18.961-13.461-14.5-13.461-33.346 0-19.076 13.884-33.23 13.884-14.153 33.115-10.922 136.769 15.384 234.384 112.999 97.615 97.615 112.999 234.384 3.231 19.23-10.538 33.115Q505.461-120 484.385-120Z"/></svg>
      </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/about">About</a>
    
    
      <a class="nav-dropdown-link" href="/atom.xml" title="RSS 订阅">RSS</a>
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/images/avatar.jpg></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">迪多 </div>
      <div class="dot"></div>
      <div class="subtitle">枝江月亮，将我照亮；<br>枝江太阳，暖我心房。 </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://twitter.com" title="Twitter"><i class="fa-brands fa-twitter"></i></a>
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://store.steampowered.com" title="Steam"><i class="fa-brands fa-steam"></i></a>
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
    
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">最新文章</h3>
      <ul>
        
          <a class="recent-link" href="/2025/11/04/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88Auto-encoder%EF%BC%89/" title="Auto-encoder（自编码器）" >
            <div class="recent-link-text">
              Auto-encoder（自编码器）
            </div>
          </a>
        
          <a class="recent-link" href="/2025/11/04/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0/" title="自监督式学习" >
            <div class="recent-link-text">
              自监督式学习
            </div>
          </a>
        
          <a class="recent-link" href="/2025/10/30/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C(GAN)/" title="生成式对抗网络(GAN)" >
            <div class="recent-link-text">
              生成式对抗网络(GAN)
            </div>
          </a>
        
          <a class="recent-link" href="/2025/10/29/Homework%203%20%20Image%20Classification/" title="Homework  3 Image Classification" >
            <div class="recent-link-text">
              Homework  3 Image Classification
            </div>
          </a>
        
          <a class="recent-link" href="/2025/10/27/Batch%20Normalization/" title="Batch Normalization（批次标准化）" >
            <div class="recent-link-text">
              Batch Normalization（批次标准化）
            </div>
          </a>
        
      </ul>
    </div>
  </div>

  
  <div class="sticky">
    
      


  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">分类</h3>
      <div class="category-box">
            <a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                学习笔记
                <div class="category-count">16</div>
            </a>
        
            <a class="category-link" href="/categories/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/">
                个人博客
                <div class="category-count">1</div>
            </a>
        </div>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">标签</h3>
      <ul class="widget-tag-list" itemprop="keywords"><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%B5%8B%E8%AF%95/" rel="tag">测试</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
    </div>
  </div>


    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-自监督式学习" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        自监督式学习
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-11-04T09:44:44.000Z" itemprop="datePublished">2025-11-04</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
    <a class="meta-cate-link" href="/categories/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/">个人博客</a>><a class="meta-cate-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
  
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            4.4k 词 
          </div>
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <p><strong>Self-supervised Learning</strong></p>
<p>supervised Learning: model的输入是x，输出是y；为了使model有想要的y，我们需要label。</p>
<p>Self-supervised Learning：自己想办法做supervise，在没有label的情况下。在没有标注的资料里面，将资料分成两部分，让一部分作为模型的输入，另一部分作为模型的标注。</p>
<p><img src="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102144607127.png" style="zoom: 33%;" /></p>
<p>有label就是supervised Learning，没有label就是unsupervised Learning。</p>
<p><strong>BERT的Self-supervised Learning</strong></p>
<p>BERT在训练的时候一般会做两个方法：Masking Input 和 Next Sentence Prediction</p>
<p>BERT是 Transformer Encoder 的一种具体实现。</p>
<p>BERT一般用在自然语言处理上，所以BERT的输入一般是一排文字。</p>
<p>接下来，我们会从 BERT 的输入文本中选择一部分词，并将它们随机的“盖住”。</p>
<p>盖住有两种做法：</p>
<ol>
<li>把句子里的某一个字换成一个特殊的符号。</li>
<li>随机把某一个字换成另外一个字。</li>
</ol>
<p>盖住以后，同样是输入一个sequence，BERT的输出对应的就是另一个sequence。</p>
<p>接下来把盖住部分对应的输出，做一个Linear transform(线性变换，就是乘一个矩阵，并且不要求该矩阵可逆)，再做SoftMax得到一个输出分布。</p>
<p>输出是一个很长的向量，向量中包含能输出的所有字对应的分数。</p>
<p>BERT训练的目标就是其输出和原来的字越接近越好(cross entropy)。</p>
<p>在训练的时候，我们训练BERT的参数，和Linear transform所用矩阵的参数。</p>
<p><img src="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102150433520.png" style="zoom:33%;" /></p>
<p>Next Sentence Prediction</p>
<p>从资料库中拿出两个句子，在两个句子中间加入一个特殊的符号[SEP]，代表分隔，让BERT知道这是两个不同的句子；然后在整个sequence的最前面加入一个特殊的符号[CLS]。</p>
<p>我们把这个sequence全部丢入到BERT里面，只取[CLS]所对应的输出，将这个输出乘上一个Linear的Transform，做一个二元分类问题，输出yes或no，预测这两个句子是不是相接的，我们将其训练成遇到相接输出yes，遇到不相接输出no。</p>
<p>但是后来的研究发现，Next Sentence Prediction对接下来BERT要做的事帮助不大，可能是预测两个句子是否接在一起这个任务过于简单了。</p>
<p>后来有另一个方法看起来是比较有用的，Sentence Order Prediction，我们的两个sentence本来就接在一起，然后我们随机更改两个句子的顺序，问BERT原来的顺序是什么样子。</p>
<p><img src="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102151859368.png" style="zoom:33%;" /></p>
<p>给BERT一些有标注的资料，其就可以解决各式各样的Downstream Tasks，这个行为称为<strong>Fine-tune</strong>。在Fine-tune之前产生BERT的过程叫做<strong>Pre-train</strong>。</p>
<p><strong>GLUE（General Language Understanding Evaluation）</strong></p>
<p>评测BERT模型的好坏，可以将其分别微调在九个任务上，看看九个任务的平均正确率是多少，这个数值代表了Self-supervised Model的好坏。</p>
<blockquote>
<p><strong>GLUE 九个任务：</strong></p>
<ul>
<li><strong>CoLA</strong> — 语言可接受性判断（Corpus of Linguistic Acceptability）</li>
<li><strong>SST-2</strong> — 情感分析（Stanford Sentiment Treebank v2）</li>
<li><strong>MRPC</strong> — 句子语义等价判断（Microsoft Research Paraphrase Corpus）</li>
<li><strong>QQP</strong> — 问句匹配（Quora Question Pairs）</li>
<li><strong>STS-B</strong> — 语义文本相似度（Semantic Textual Similarity Benchmark）</li>
<li><strong>MNLI</strong> — 多领域自然语言推理（Multi-Genre Natural Language Inference）</li>
<li><strong>QNLI</strong> — 问答式自然语言推理（Question Natural Language Inference）</li>
<li><strong>RTE</strong> — 文本蕴含识别（Recognizing Textual Entailment）</li>
<li><strong>WNLI</strong> — 常识指代推理（Winograd Natural Language Inference）</li>
</ul>
</blockquote>
<p><strong>How to use BERT - Case1</strong></p>
<p>输入：sequence</p>
<p>输出：class</p>
<p>例子：情感分析判断句子是正面还是负面</p>
<p>给BERT一个句子，在句子的前面放入[CLS]的token，得到[CLS]输出的向量，将其做Linear Transform并进行SoftMax来决定正面或负面。</p>
<p>实际在做的时候，我们需要下游任务的标注资料（大量的句子和正负面的标签），才能训练这个模型。</p>
<p>在训练的时候，我们把Linear Transform使用的矩阵和BERT合起来才是完整的Sentiment Classification模型。</p>
<p>在训练的时候，Linear的矩阵和BERT都会使用梯度下降法来update。</p>
<p>但是Linear的部分是随机初始化的，BERT部分的初始参数是进行过Pre-train的。</p>
<p>我们在做BERT的时候用的是没有标注的资料，在下游任务用了少量有标注的资料，合起来就是semi-supervise。</p>
<p><img src="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102154437006.png" style="zoom:33%;" /></p>
<p>如果没有Pre-train的话，在训练时Loss下降比较慢，并且就算训练到最后，没有Pre-train的Loss还是要比有Pre-train的Loss要高。</p>
<p><strong>How to use BERT - Case2</strong></p>
<p>输入：sequence</p>
<p>输出：sequence</p>
<p>例子：词性标注</p>
<p>给BERT输入一个句子（前面有[CLS]），这个句子的每一个Token都有一个对应的向量，接下来再把每一个向量做一个Linear transform，然后再做SoftMax，最后得出输入的每一个词汇属于哪一个类别。</p>
<p>接下来仍然是使用少量带有Label的资料来训练整个模型。</p>
<p><img src="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102155914445.png" style="zoom:33%;" /></p>
<p><strong>How to use BERT - Case3</strong></p>
<p>输入：two sequences</p>
<p>输出：a class</p>
<p>例子：自然语言推理（NLI，Natural Language Inference）</p>
<p>NLI所做的事：给机器两个句子，一个是前提，一个是假设；机器要做的事就是看能不能根据前提来推出这个假设。</p>
<p>contradiction entailment neutral - 矛盾 蕴含 中立</p>
<p>蕴含：前提为真，假设必须为真。</p>
<p>中立：前提不足以确认假设真伪。</p>
<p>矛盾：前提为真时，假设一定为假。</p>
<p>给BERT两个句子，两个句子之间放一个[SEP]，最前面放[CLS]；一整串直接丢进BERT里面，BERT也会输出对应的长度，但是我们只取[CLS]的输出，接下来进行Linear Transform和SoftMax，最后给出类别，判断这个两个句子是不是矛盾的。</p>
<p><img src="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102161233548.png" style="zoom:33%;" /></p>
<p><strong>How to use BERT - Case4</strong></p>
<p>Extraction-based Question Answering(QA)答案一定出现在文章里面的QA。</p>
<p>输入：文章和问题D Q</p>
<p>输出：两个正整数(s,e)</p>
<p>根据这两个正整数，直接从文章里面截取一个段落，从第s个字到第e个字串起来就是正确答案。</p>
<p>输入问题和文章，问题和文章中间有一个特殊的符号[SEP]，前面再放一个[CLS]的token。</p>
<p>需要从头开始训练的东西只有两个向量，向量的长度和BERT输出的Vector向量长度是一样的。</p>
<p>先将向量1拿出来，和对应文章输出的每一个向量做点乘，算出对应的数值，接下来对算出的所有数值做SoftMax，哪里分数最高，S就取什么。</p>
<p>再将向量1拿出来，和对应文章输出的每一个向量做点乘，算出对应的数值，接下来对算出的所有数值做SoftMax，哪里分数最高，E就取什么。</p>
<p><strong>Pre-training a seq2seq model</strong></p>
<p>Pretrain seq2seq的Decoder.。</p>
<p>给Encoder的输入故意做一些扰动，将其[弄坏]；Decoder的目的是使输出的句子和弄坏前一致，Train下去就是一个seq2seq的model。</p>
<p>弄坏：Mask词汇，删掉词汇，弄乱词汇，词汇旋转……</p>
<p><img src="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251102165119435.png" style="zoom:33%;" /></p>
<hr>
<p><strong>Why does BERT work?</strong></p>
<p>对于BERT而言，输入一段文字，每段文字都有一个对应的向量，我们管这个向量叫<strong>enbedding</strong>，这些向量代表了输入的那个字的意思。</p>
<p>代表意思：把这些字对应的向量画出来，意思越相近的字，它们的向量就越接近。</p>
<p>BERT能考虑上下文，所以对于相同的字，上下文不同，对应的向量也是不同的。</p>
<p>基于语言学的假设：我们知道一个词汇的意思是基于其上下文，我们在训练BERT做填空题的过程中，也许其学到的就是从上下文去抽取资讯。</p>
<p>BERT的能力不完全来自于其看得懂文章这件事，而是可能有其他的理由。</p>
<p>BERT也许本质上就是一个比较好的初始化参数，不见得和语义有关，这种参数就是特别适合拿来做大型模型的训练……..BERT是如何起作用的还有很大的研究空间。</p>
<p><strong>多语言的BERT(Multi-lingual BERT)</strong></p>
<p>训练BERT的时候使用不同的语言。</p>
<p>有一个Multi-lingual BERT，使用104种语言做成，所以其会做104种语言的填空题。</p>
<p>Multi-lingual BERT拿英文的QA资料去训练，它自动就会学会做中文QA的问题。</p>
<p>一个比较简单的解释：也许对于Multi-lingual BERT而言，不同语言间没有什么差异。不论是中文还是英文，只要是相同词汇的embedding都会很近。</p>
<p><img src="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251104171233458.png" style="zoom:33%;" /></p>
<p><strong>Weird???</strong></p>
<p>BERT可以把不同语言同样意思的向量让他们很接近，可是再做填空题的时候，给中文做的是中文填空；给英文做的是英文填空，它们不会混在一起。</p>
<p>为什么给中文的时候不会填写英文进去呢？</p>
<p>代表BERT知道语言的资讯对于它来说不同，来自不同语言的那些符号终究还是不一样。</p>
<p><img src="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251104171805144.png" style="zoom:33%;" /></p>
<hr>
<p><strong>GPT series</strong></p>
<p>GPT要做的任务是，预测接下来会出现的token是什么。</p>
<p>给GPT一个<BOS>token ，然后GPT output 一个embedding，然后用这个embedding去预测下一个应该出现的token是什么。</p>
<p>详细来看：用一个embedding进行Linear Transform，之后再使用softmax得到一个distribution，接下来我们希望output出来的distribution和正确答案的Cross entropy越小越好。</p>
<p>接下来就是以此类推。</p>
<p><img src="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251104172713698.png" style="zoom:33%;" /></p>
<p>GPT厉害的地方就是用了很多资料，训了一个异常巨大的模型。</p>
<p>GPT的模型像是Transformer的Decoder，也就是我们做的Mask的intention。</p>
<p><strong>How to use GPT?</strong></p>
<p>怎么能把一句话补全的能力用在Downstream的任务上呢？</p>
<p>举例来说，怎么把其用在QA，或者是其他的和NLP有关的任务呢？</p>
<p>假设要用GPT这个模型做翻译：先给任务的描述(task description)，然后给其几个例子(examples)，接下来再给Prompt。</p>
<p><img src="https://diduoblog-pics.oss-cn-beijing.aliyuncs.com/20251104173554388.png" style="zoom: 33%;" /></p>
<p>GPT的训练不适用梯度下降法，是一种特殊的Learning，称为”in-context” Learning。</p>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/2025/11/04/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88Auto-encoder%EF%BC%89/"
      title="Auto-encoder（自编码器）"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        Auto-encoder（自编码器）
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/2025/10/30/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C(GAN)/"
      title="生成式对抗网络(GAN)"
     >

    <p class="title-text">
      
        生成式对抗网络(GAN)
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>






    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2025 迪多<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
</body>
</html>
